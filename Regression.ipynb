{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T18:29:27.612765Z","iopub.execute_input":"2022-06-12T18:29:27.613759Z","iopub.status.idle":"2022-06-12T18:29:27.631651Z","shell.execute_reply.started":"2022-06-12T18:29:27.613700Z","shell.execute_reply":"2022-06-12T18:29:27.630410Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import Python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:27.634991Z","iopub.execute_input":"2022-06-12T18:29:27.636037Z","iopub.status.idle":"2022-06-12T18:29:28.727017Z","shell.execute_reply.started":"2022-06-12T18:29:27.635986Z","shell.execute_reply":"2022-06-12T18:29:28.725901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:28.728189Z","iopub.execute_input":"2022-06-12T18:29:28.728526Z","iopub.status.idle":"2022-06-12T18:29:28.733190Z","shell.execute_reply.started":"2022-06-12T18:29:28.728495Z","shell.execute_reply":"2022-06-12T18:29:28.732247Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#create a list of columns for a dataset\ncolumn_names = ['CRIM', 'ZN','INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\npath = '../input/boston-house-prices/housing.csv'\n# read the dataset\ndf = pd.read_csv(path, delim_whitespace=True, header = None, names = column_names)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:28.735653Z","iopub.execute_input":"2022-06-12T18:29:28.736425Z","iopub.status.idle":"2022-06-12T18:29:28.767645Z","shell.execute_reply.started":"2022-06-12T18:29:28.736377Z","shell.execute_reply":"2022-06-12T18:29:28.766618Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train, validation, test = np.split(df.sample(frac=1), [int(0.6*len(df)),\nint(.8*len(df))])\nprint(train)\nprint(test)\nprint(validation)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:28.769513Z","iopub.execute_input":"2022-06-12T18:29:28.769914Z","iopub.status.idle":"2022-06-12T18:29:28.821405Z","shell.execute_reply.started":"2022-06-12T18:29:28.769882Z","shell.execute_reply":"2022-06-12T18:29:28.820412Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Assign features and target variables to X and y\nX = df.RM.values\ny = df.MEDV.values\n\n# Setup learning rate\nl_rate = 0.00001\n\n# Setup a number of iterations \nn_iters = 5000\n\n# Assign a random integer to a slope and intercept\nslope, intercept = np.random.rand(), np.random.rand()\n\n# Store a number of feature values\nlength =len(X)\n\n# Create starting point for calculation of partial derivatives\nslope_deriv, intercept_deriv = 0, 0\n    \n# Create empty lists for storing the output of each iteration\ncost_list, slope_list, intercept_list = [], [], []\n\n# State the learning rate\nlearning_rate = l_rate\n\n# State the number of iterations \nfor i in range (n_iters):\n    #formulate a linear equation\n    y_pred = slope * X + intercept\n\n    # cost function\n    cost = 0\n    for i in range(length):\n        cost += ((slope * X[i] + intercept) - y[i]) ** 2\n    cost = cost / length\n    # Calculate partial derivatives of a cost function with respect to slope and to intercept\n    for i in range(length):\n        slope_deriv += ((slope * X[i] + intercept) - y[i]) * 2 * X[i]\n        intercept_deriv += (slope * X[i] + intercept) - y[i] \n    # Update slope and intercept\n    slope -= 1/length * learning_rate * slope_deriv\n    intercept -= 1/length * learning_rate * intercept_deriv \n    # Appending lists for future processing\n    slope_list.append(slope)\n    intercept_list.append(intercept)\n    cost_list.append(cost)\n \n\n\n   # reg=None\n    \n\n   # if reg is not None:\n    \n    #    if reg == 'L1':  # For Lasso Regression (L1), add the magnitudes\n     #       cost += lmda * sum(abs(beta))\n      #  elif reg == 'L2':  # For Ridge Regression (L2), add the squared magnitude\n       #     cost += lmda * sum(beta**2)\n    #return cost","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:28.823243Z","iopub.execute_input":"2022-06-12T18:29:28.823998Z","iopub.status.idle":"2022-06-12T18:29:40.296108Z","shell.execute_reply.started":"2022-06-12T18:29:28.823949Z","shell.execute_reply":"2022-06-12T18:29:40.295177Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Calculate min of the cost function\ncost_min = cost_list.index(np.min(cost_list))\nprint('The minimum of the cost function is {:.2f}, which can be found at {} iteration'.format(np.min(cost_list), cost_min))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.297217Z","iopub.execute_input":"2022-06-12T18:29:40.297907Z","iopub.status.idle":"2022-06-12T18:29:40.304661Z","shell.execute_reply.started":"2022-06-12T18:29:40.297870Z","shell.execute_reply":"2022-06-12T18:29:40.303710Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# visualize the cost function and its minimum\ndef cost_plot():\n    plt.plot(cost_list, label = 'Cost function')\n    plt.scatter(cost_list.index(np.min(cost_list)), np.min(cost_list),\n            color = 'red', marker = 'x', s = 70, label = 'Minimum')\n    plt.xlabel('Iterations')\n    plt.legend()\n    plt.ylim(0, np.max(cost_list) + 100)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.305797Z","iopub.execute_input":"2022-06-12T18:29:40.306180Z","iopub.status.idle":"2022-06-12T18:29:40.376681Z","shell.execute_reply.started":"2022-06-12T18:29:40.306149Z","shell.execute_reply":"2022-06-12T18:29:40.375672Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Make some additions to visualization of the cost function and the minimum\nplt.subplots(figsize=(16, 4))\nplt.subplot(1,2,1)\ncost_plot()\nplt.ylabel('Value of cost function')\nplt.title('Cost function with its minimum')\nplt.xlim(0, n_iters)\nplt.subplot(1,2,2)\ncost_plot()\nplt.title('Cost function with its minimum zoomed in')\nplt.xlim(cost_min -100, cost_min +100);","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.378986Z","iopub.execute_input":"2022-06-12T18:29:40.379324Z","iopub.status.idle":"2022-06-12T18:29:40.836688Z","shell.execute_reply.started":"2022-06-12T18:29:40.379293Z","shell.execute_reply":"2022-06-12T18:29:40.835782Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression().fit(X.reshape(-1,1), y.reshape(-1, 1))\npred = model.predict(X.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.837662Z","iopub.execute_input":"2022-06-12T18:29:40.838416Z","iopub.status.idle":"2022-06-12T18:29:40.852137Z","shell.execute_reply.started":"2022-06-12T18:29:40.838383Z","shell.execute_reply":"2022-06-12T18:29:40.851388Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def single_regr_plot(prediction):\n    plt.scatter(X, y, color = '#20B648',\n            marker = 'o', edgecolor = '#56798B', alpha = 0.7, label = 'Original Data')\n\n    plt.plot(X, prediction, color = '#15029A', label = 'Prediction')\n    plt.xlabel(\"Average Number of Rooms per Dwelling\")\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.855933Z","iopub.execute_input":"2022-06-12T18:29:40.856864Z","iopub.status.idle":"2022-06-12T18:29:40.862647Z","shell.execute_reply.started":"2022-06-12T18:29:40.856823Z","shell.execute_reply":"2022-06-12T18:29:40.861592Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Plot linear regression\nplt.subplots(figsize=(16, 4))\nplt.subplot(1,2,1)\nsingle_regr_plot(slope_list[cost_min] * X + intercept_list[cost_min])\nplt.ylabel(\"Median value of owner-occupied homes in $1000's\")\nplt.title('Manual model')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:40.864009Z","iopub.execute_input":"2022-06-12T18:29:40.864643Z","iopub.status.idle":"2022-06-12T18:29:41.133066Z","shell.execute_reply.started":"2022-06-12T18:29:40.864562Z","shell.execute_reply":"2022-06-12T18:29:41.132308Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X.reshape(-1, 1))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.134222Z","iopub.execute_input":"2022-06-12T18:29:41.134658Z","iopub.status.idle":"2022-06-12T18:29:41.139665Z","shell.execute_reply.started":"2022-06-12T18:29:41.134626Z","shell.execute_reply":"2022-06-12T18:29:41.138637Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(poly_features, y)\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.141006Z","iopub.execute_input":"2022-06-12T18:29:41.141335Z","iopub.status.idle":"2022-06-12T18:29:41.158162Z","shell.execute_reply.started":"2022-06-12T18:29:41.141305Z","shell.execute_reply":"2022-06-12T18:29:41.157379Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title(\"Your first polynomial regression – congrats! :)\", size=16)\nplt.scatter(X, y)\nplt.scatter(X, y_predicted, c=\"red\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.159227Z","iopub.execute_input":"2022-06-12T18:29:41.159934Z","iopub.status.idle":"2022-06-12T18:29:41.379409Z","shell.execute_reply.started":"2022-06-12T18:29:41.159883Z","shell.execute_reply":"2022-06-12T18:29:41.378449Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\narray = df.values\nX = array[:,0:13]\nY = array[:,13]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\nridgeReg = Ridge(alpha=20.0).fit(X_train,y_train)\n\nprint (\"Training Score - {:.3f}\".format(ridgeReg.score(X_train,y_train)))\nprint (\"Testing Score - {:.3f}\".format(ridgeReg.score(X_test,y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(ridgeReg.coef_ != 0)))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.380620Z","iopub.execute_input":"2022-06-12T18:29:41.380991Z","iopub.status.idle":"2022-06-12T18:29:41.397649Z","shell.execute_reply.started":"2022-06-12T18:29:41.380958Z","shell.execute_reply":"2022-06-12T18:29:41.396888Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#apply  Normalization to Ridge regression\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n#X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n\nX_train_scale = scaler.fit_transform(X_train)\nX_test_scale = scaler.transform(X_test)\n\nridgeReg = Ridge(alpha=20.0).fit(X_train_scale,y_train)\n\nprint (\"Training Score - {:.3f}\".format(ridgeReg.score(X_train_scale,y_train)))\nprint (\"Testing Score - {:.3f}\".format(ridgeReg.score(X_test_scale,y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(ridgeReg.coef_ != 0)))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.398730Z","iopub.execute_input":"2022-06-12T18:29:41.399581Z","iopub.status.idle":"2022-06-12T18:29:41.413161Z","shell.execute_reply.started":"2022-06-12T18:29:41.399531Z","shell.execute_reply":"2022-06-12T18:29:41.412381Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for newAlpha in [0, 1, 10, 20, 50, 100, 1000]:\n    linridge = Ridge(alpha = newAlpha).fit(X_train_scale, y_train)\n    r2_train = linridge.score(X_train_scale, y_train)\n    r2_test = linridge.score(X_test_scale, y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\nr-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n         .format(newAlpha, num_coeff_bigger, r2_train, r2_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T18:29:41.414137Z","iopub.execute_input":"2022-06-12T18:29:41.414949Z","iopub.status.idle":"2022-06-12T18:29:41.435166Z","shell.execute_reply.started":"2022-06-12T18:29:41.414911Z","shell.execute_reply":"2022-06-12T18:29:41.434475Z"},"trusted":true},"execution_count":19,"outputs":[]}]}